{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a73c31e",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51a4e7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "%matplotlib inline\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Modelling\n",
    "from statsmodels.tsa.api import VAR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.base import  datetools\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f9bd3b6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>record</th>\n",
       "      <th>bioTotal</th>\n",
       "      <th>ecoTotal</th>\n",
       "      <th>countryCode</th>\n",
       "      <th>bioFootprint</th>\n",
       "      <th>ecoFootprint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>1961-01-01</td>\n",
       "      <td>cropLand</td>\n",
       "      <td>4.990785e+06</td>\n",
       "      <td>1.010593e+07</td>\n",
       "      <td>002</td>\n",
       "      <td>1.252687e+07</td>\n",
       "      <td>2.536589e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>1961-01-01</td>\n",
       "      <td>fishingGround</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>9.706928e+02</td>\n",
       "      <td>002</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.591563e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>1961-01-01</td>\n",
       "      <td>grazingLand</td>\n",
       "      <td>6.212850e+06</td>\n",
       "      <td>1.064433e+07</td>\n",
       "      <td>002</td>\n",
       "      <td>2.857911e+06</td>\n",
       "      <td>4.896393e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>1961-01-01</td>\n",
       "      <td>builtupLand</td>\n",
       "      <td>2.722616e+05</td>\n",
       "      <td>5.445231e+05</td>\n",
       "      <td>002</td>\n",
       "      <td>6.833765e+05</td>\n",
       "      <td>1.366753e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>1961-01-01</td>\n",
       "      <td>carbon</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.596314e+05</td>\n",
       "      <td>002</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.051356e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       country       year         record      bioTotal      ecoTotal  \\\n",
       "0  Afghanistan 1961-01-01       cropLand  4.990785e+06  1.010593e+07   \n",
       "1  Afghanistan 1961-01-01  fishingGround  0.000000e+00  9.706928e+02   \n",
       "2  Afghanistan 1961-01-01    grazingLand  6.212850e+06  1.064433e+07   \n",
       "3  Afghanistan 1961-01-01    builtupLand  2.722616e+05  5.445231e+05   \n",
       "4  Afghanistan 1961-01-01         carbon  0.000000e+00  5.596314e+05   \n",
       "\n",
       "  countryCode  bioFootprint  ecoFootprint  \n",
       "0         002  1.252687e+07  2.536589e+07  \n",
       "1         002  0.000000e+00  3.591563e+02  \n",
       "2         002  2.857911e+06  4.896393e+06  \n",
       "3         002  6.833765e+05  1.366753e+06  \n",
       "4         002  0.000000e+00  7.051356e+05  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = 'Data/Modelling/PredictionDataset.csv'\n",
    "\n",
    "df = pd.read_csv(filepath, encoding = 'unicode_escape')\n",
    "df = df.drop(['Unnamed: 0'], axis = 1)\n",
    "# Dropping countries whose code is NA\n",
    "df = df[df['countryCode'].notna()]\n",
    "\n",
    "# Making the country codes 3 digits long\n",
    "df.countryCode = df[['countryCode']].astype(int)\n",
    "codes = list(df.countryCode)\n",
    "a = [\"%03d\" % x for x in (codes)]\n",
    "df['countryCode'] = a\n",
    "    \n",
    "df['year']= pd.to_datetime(df['year'], format='%Y')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a7712c",
   "metadata": {},
   "source": [
    "### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ee8babc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def navalues(data):\n",
    "    cols = data.columns\n",
    "    for j in cols:\n",
    "        for i in range(0,len(data)):\n",
    "             if data[j][i].isna():\n",
    "                data[j][i] = data[j][i-1]\n",
    "                \n",
    "def scale(df):\n",
    "    sc = StandardScaler()\n",
    "    df[['bioTotal', 'ecoTotal', 'bioFootprint', 'ecoFootprint']] = sc.fit_transform(df[['bioTotal', 'ecoTotal', 'bioFootprint', 'ecoFootprint']])\n",
    "    return df\n",
    "\n",
    "def reverse_scale(df):\n",
    "    sc = StandardScaler()\n",
    "    return sc.inverse_transform(df)\n",
    "\n",
    "def encode(df):\n",
    "    if 'country' in df.columns.unique():\n",
    "        df = df.drop(['country'], axis=1)\n",
    "    if 'record' in df.columns.unique():\n",
    "        df = pd.get_dummies(df, columns=[\"record\"])\n",
    "    return df\n",
    "\n",
    "def prepare_timeseries(df, record):\n",
    "    return df.set_index(['year'])\n",
    "\n",
    "def prepareCountryDf(df, countryCode):\n",
    "    df = df.loc[df['countryCode'] == countryCode]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11ad3b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bioTotal</th>\n",
       "      <th>ecoTotal</th>\n",
       "      <th>countryCode</th>\n",
       "      <th>bioFootprint</th>\n",
       "      <th>ecoFootprint</th>\n",
       "      <th>record_builtupLand</th>\n",
       "      <th>record_carbon</th>\n",
       "      <th>record_cropLand</th>\n",
       "      <th>record_fishingGround</th>\n",
       "      <th>record_forestLand</th>\n",
       "      <th>record_grazingLand</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1961-01-01</th>\n",
       "      <td>-0.128429</td>\n",
       "      <td>-0.139998</td>\n",
       "      <td>002</td>\n",
       "      <td>-0.067686</td>\n",
       "      <td>-0.113729</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-01-01</th>\n",
       "      <td>-0.200220</td>\n",
       "      <td>-0.183028</td>\n",
       "      <td>002</td>\n",
       "      <td>-0.188002</td>\n",
       "      <td>-0.192250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-01-01</th>\n",
       "      <td>-0.110850</td>\n",
       "      <td>-0.137705</td>\n",
       "      <td>002</td>\n",
       "      <td>-0.160552</td>\n",
       "      <td>-0.177094</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-01-01</th>\n",
       "      <td>-0.196303</td>\n",
       "      <td>-0.180714</td>\n",
       "      <td>002</td>\n",
       "      <td>-0.181438</td>\n",
       "      <td>-0.188020</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-01-01</th>\n",
       "      <td>-0.200220</td>\n",
       "      <td>-0.180649</td>\n",
       "      <td>002</td>\n",
       "      <td>-0.188002</td>\n",
       "      <td>-0.190068</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01</th>\n",
       "      <td>-0.198170</td>\n",
       "      <td>-0.182060</td>\n",
       "      <td>181</td>\n",
       "      <td>-0.187495</td>\n",
       "      <td>-0.191989</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01</th>\n",
       "      <td>-0.137533</td>\n",
       "      <td>-0.145716</td>\n",
       "      <td>181</td>\n",
       "      <td>-0.168748</td>\n",
       "      <td>-0.179773</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01</th>\n",
       "      <td>-0.168524</td>\n",
       "      <td>-0.155124</td>\n",
       "      <td>181</td>\n",
       "      <td>-0.134881</td>\n",
       "      <td>-0.141329</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01</th>\n",
       "      <td>-0.195400</td>\n",
       "      <td>-0.180179</td>\n",
       "      <td>181</td>\n",
       "      <td>-0.179924</td>\n",
       "      <td>-0.187044</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01</th>\n",
       "      <td>-0.200220</td>\n",
       "      <td>-0.130263</td>\n",
       "      <td>181</td>\n",
       "      <td>-0.188002</td>\n",
       "      <td>-0.143916</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40296 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            bioTotal  ecoTotal countryCode  bioFootprint  ecoFootprint  \\\n",
       "year                                                                     \n",
       "1961-01-01 -0.128429 -0.139998         002     -0.067686     -0.113729   \n",
       "1961-01-01 -0.200220 -0.183028         002     -0.188002     -0.192250   \n",
       "1961-01-01 -0.110850 -0.137705         002     -0.160552     -0.177094   \n",
       "1961-01-01 -0.196303 -0.180714         002     -0.181438     -0.188020   \n",
       "1961-01-01 -0.200220 -0.180649         002     -0.188002     -0.190068   \n",
       "...              ...       ...         ...           ...           ...   \n",
       "2018-01-01 -0.198170 -0.182060         181     -0.187495     -0.191989   \n",
       "2018-01-01 -0.137533 -0.145716         181     -0.168748     -0.179773   \n",
       "2018-01-01 -0.168524 -0.155124         181     -0.134881     -0.141329   \n",
       "2018-01-01 -0.195400 -0.180179         181     -0.179924     -0.187044   \n",
       "2018-01-01 -0.200220 -0.130263         181     -0.188002     -0.143916   \n",
       "\n",
       "            record_builtupLand  record_carbon  record_cropLand  \\\n",
       "year                                                             \n",
       "1961-01-01                   0              0                1   \n",
       "1961-01-01                   0              0                0   \n",
       "1961-01-01                   0              0                0   \n",
       "1961-01-01                   1              0                0   \n",
       "1961-01-01                   0              1                0   \n",
       "...                        ...            ...              ...   \n",
       "2018-01-01                   0              0                0   \n",
       "2018-01-01                   0              0                0   \n",
       "2018-01-01                   0              0                1   \n",
       "2018-01-01                   1              0                0   \n",
       "2018-01-01                   0              1                0   \n",
       "\n",
       "            record_fishingGround  record_forestLand  record_grazingLand  \n",
       "year                                                                     \n",
       "1961-01-01                     0                  0                   0  \n",
       "1961-01-01                     1                  0                   0  \n",
       "1961-01-01                     0                  0                   1  \n",
       "1961-01-01                     0                  0                   0  \n",
       "1961-01-01                     0                  0                   0  \n",
       "...                          ...                ...                 ...  \n",
       "2018-01-01                     1                  0                   0  \n",
       "2018-01-01                     0                  0                   1  \n",
       "2018-01-01                     0                  0                   0  \n",
       "2018-01-01                     0                  0                   0  \n",
       "2018-01-01                     0                  0                   0  \n",
       "\n",
       "[40296 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_others = encode(scale(df))\n",
    "df_others.index = df_others.year\n",
    "df_others = df_others.drop(['year'], 1)\n",
    "df_others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e1720c",
   "metadata": {},
   "source": [
    "## Prediction Models\n",
    "\n",
    "### 1. Hist Gradient Bossting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47ee38c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df_others, test_size=0.2)\n",
    "\n",
    "X_train = train.iloc[:,[0,2]]\n",
    "y_train = train[\"ecoFootprint\"]\n",
    "X_test = test.iloc[:,[0,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7553ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "regressor = HistGradientBoostingRegressor(min_samples_leaf=1)\n",
    "regressor.fit(X_train, y_train)\n",
    "test[\"predictions\"] = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86227011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "# ax.plot(train.index,train[\"ecoFootprint\"], label='train data', color=\"green\")\n",
    "# ax.plot(test.index,test[\"ecoFootprint\"], label='test data', color=\"blue\")\n",
    "# ax.scatter(test.index, test[\"predictions\"], label='predictions', color=\"red\", s=75)\n",
    "# ax.legend(loc=\"upper left\")\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd312162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.2334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print('MSE = %.4g' % mean_squared_error(test[\"ecoFootprint\"],test[\"predictions\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cf420f",
   "metadata": {},
   "source": [
    "### 2. Linear Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c0c17d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q --upgrade linear-tree\n",
    "\n",
    "from lineartree import LinearBoostRegressor\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5278e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearBoostRegressor(base_estimator=LinearRegression(),\n",
    "                                 n_estimators = 300, \n",
    "                                 max_depth    = 5,\n",
    "                                 random_state = 42)\n",
    "regressor.fit(X_train, y_train)\n",
    "test[\"predictions\"] = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30678cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "# ax.plot(train.index,train[\"ecoFootprint\"], label='train data', color=\"green\")\n",
    "# ax.plot(test.index,test[\"ecoFootprint\"], label='test data', color=\"blue\")\n",
    "# ax.scatter(test.index, test[\"predictions\"], label='predictions', color=\"red\", s=75)\n",
    "# ax.legend(loc=\"upper left\")\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "681f90e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.2347\n"
     ]
    }
   ],
   "source": [
    "print('MSE = %.4g' % mean_squared_error(test[\"ecoFootprint\"],test[\"predictions\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b86242a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.style.use('fivethirtyeight')\n",
    "# plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "# !pip install -q ptitprince\n",
    "# import ptitprince as pt\n",
    "\n",
    "# mono_font = {'fontname':'monospace'}\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(15, 5))\n",
    "# pt.RainCloud(data=train_data, y = \"ecoFootprint\", bw=0.1, cut=0, hue=['teal'], orient='h', label=\"training data\", palette=['teal'], alpha = .65)\n",
    "# pt.RainCloud(data=test_data, y = \"predictions\", bw=0.1, cut=0, orient='h',label=\"predictions\", hue=['crimson'], palette=['crimson'], alpha = .65)\n",
    "# plt.title(\"Indoor_temperature_room\", fontsize=18, **mono_font)\n",
    "# plt.xlabel(\"Temperature\")\n",
    "# legend = plt.legend()\n",
    "# plt.tick_params(labelleft=False, left=False)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f932a2",
   "metadata": {},
   "source": [
    "### 3. Tabular Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc9268b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44618349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is all columns except target var and y is target var\n",
    "X = df_others.drop([\"ecoFootprint\"], 1)\n",
    "y = df_others[\"ecoFootprint\"]\n",
    "y = np.asarray(y).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a050faed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train - Test\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, random_state=69)\n",
    "\n",
    "# Split train into train-val\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.1, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0c4cf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test, y_val = y_train.astype('float32'), y_test.astype(float), y_val.astype(float)\n",
    "X_train = np.asarray(X_train).astype('float32')\n",
    "X_val = np.asarray(X_val).astype('float32')\n",
    "X_test = np.asarray(X_test).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865e0062",
   "metadata": {},
   "source": [
    "- The <code> torch.from_numpy </code> module helps to convert the numpy array into a tensor in PyTorch.\n",
    "- The returned tensor and initial numpy array share the same memory. However, the tensor is not resizeable.\n",
    "- <code> RegressionDataset </code> is a custom class that uses PyTorch's Dataset module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a1b5b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionDataset(Dataset):\n",
    "    # Initializing the data that we will be using\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "    \n",
    "    # Gets a sample of the data being passed and constructs the dataset\n",
    "    # Hence it will loop through the function and create a sample from each instance in the dataset\n",
    "    # index is the data instance being looped through\n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    # Returns the length of the data\n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "train_dataset = RegressionDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float())\n",
    "val_dataset = RegressionDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).float())\n",
    "test_dataset = RegressionDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed47c3f0",
   "metadata": {},
   "source": [
    "- First, we initialize the model parameters\n",
    "- Then, making use of Torch's DataLoader module to pass samples in <b> mini batches. </b> This ensures that the data is reshuffled at every epoch to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b41ec699",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 150\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_FEATURES = len(X.columns)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=1)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e61f08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleRegression(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(MultipleRegression, self).__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Linear(num_features, 16)\n",
    "        self.layer_2 = nn.Linear(16, 32)\n",
    "        self.layer_3 = nn.Linear(32, 16)\n",
    "        self.layer_out = nn.Linear(16, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.relu(self.layer_3(x))\n",
    "        x = self.layer_out(x)\n",
    "        return (x)\n",
    "\n",
    "    def predict(self, test_inputs):\n",
    "        x = self.relu(self.layer_1(test_inputs))\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.relu(self.layer_3(x))\n",
    "        x = self.layer_out(x)\n",
    "        return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cefe2b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "###################### OUTPUT ######################\n",
    "cuda:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbc64123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultipleRegression(\n",
      "  (layer_1): Linear(in_features=10, out_features=16, bias=True)\n",
      "  (layer_2): Linear(in_features=16, out_features=32, bias=True)\n",
      "  (layer_3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (layer_out): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MultipleRegression(NUM_FEATURES)\n",
    "model.to(device)\n",
    "print(model)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b61d46dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_stats = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a034372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c862fe38e8c646ad9aef88c0add99d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 0.65521 | Val Loss: 0.13024\n",
      "Epoch 002: | Train Loss: 0.03187 | Val Loss: 0.00983\n",
      "Epoch 003: | Train Loss: 0.00914 | Val Loss: 0.00397\n",
      "Epoch 004: | Train Loss: 0.00538 | Val Loss: 0.00337\n",
      "Epoch 005: | Train Loss: 0.00461 | Val Loss: 0.00260\n",
      "Epoch 006: | Train Loss: 0.00439 | Val Loss: 0.00849\n",
      "Epoch 007: | Train Loss: 0.00449 | Val Loss: 0.00612\n",
      "Epoch 008: | Train Loss: 0.00415 | Val Loss: 0.00822\n",
      "Epoch 009: | Train Loss: 0.00460 | Val Loss: 0.00227\n",
      "Epoch 010: | Train Loss: 0.00564 | Val Loss: 0.00195\n",
      "Epoch 011: | Train Loss: 0.00430 | Val Loss: 0.00308\n",
      "Epoch 012: | Train Loss: 0.00399 | Val Loss: 0.00195\n",
      "Epoch 013: | Train Loss: 0.00403 | Val Loss: 0.00201\n",
      "Epoch 014: | Train Loss: 0.00283 | Val Loss: 0.00725\n",
      "Epoch 015: | Train Loss: 0.00398 | Val Loss: 0.00207\n",
      "Epoch 016: | Train Loss: 0.00368 | Val Loss: 0.00324\n",
      "Epoch 017: | Train Loss: 0.00323 | Val Loss: 0.00169\n",
      "Epoch 018: | Train Loss: 0.00614 | Val Loss: 0.00305\n",
      "Epoch 019: | Train Loss: 0.00309 | Val Loss: 0.00233\n",
      "Epoch 020: | Train Loss: 0.00217 | Val Loss: 0.00115\n",
      "Epoch 021: | Train Loss: 0.00252 | Val Loss: 0.00147\n",
      "Epoch 022: | Train Loss: 0.00425 | Val Loss: 0.00151\n",
      "Epoch 023: | Train Loss: 0.00369 | Val Loss: 0.00150\n",
      "Epoch 024: | Train Loss: 0.00259 | Val Loss: 0.00144\n",
      "Epoch 025: | Train Loss: 0.00287 | Val Loss: 0.00159\n",
      "Epoch 026: | Train Loss: 0.00280 | Val Loss: 0.00151\n",
      "Epoch 027: | Train Loss: 0.00257 | Val Loss: 0.00119\n",
      "Epoch 028: | Train Loss: 0.00186 | Val Loss: 0.00124\n",
      "Epoch 029: | Train Loss: 0.00147 | Val Loss: 0.00100\n",
      "Epoch 030: | Train Loss: 0.00230 | Val Loss: 0.00284\n",
      "Epoch 031: | Train Loss: 0.00264 | Val Loss: 0.00220\n",
      "Epoch 032: | Train Loss: 0.00186 | Val Loss: 0.00118\n",
      "Epoch 033: | Train Loss: 0.00249 | Val Loss: 0.00204\n",
      "Epoch 034: | Train Loss: 0.00290 | Val Loss: 0.00076\n",
      "Epoch 035: | Train Loss: 0.00327 | Val Loss: 0.00143\n",
      "Epoch 036: | Train Loss: 0.00249 | Val Loss: 0.00542\n",
      "Epoch 037: | Train Loss: 0.00177 | Val Loss: 0.00205\n",
      "Epoch 038: | Train Loss: 0.00158 | Val Loss: 0.00097\n",
      "Epoch 039: | Train Loss: 0.00141 | Val Loss: 0.00124\n",
      "Epoch 040: | Train Loss: 0.00168 | Val Loss: 0.00113\n",
      "Epoch 041: | Train Loss: 0.00201 | Val Loss: 0.00119\n",
      "Epoch 042: | Train Loss: 0.00211 | Val Loss: 0.01870\n",
      "Epoch 043: | Train Loss: 0.00222 | Val Loss: 0.00230\n",
      "Epoch 044: | Train Loss: 0.00172 | Val Loss: 0.00117\n",
      "Epoch 045: | Train Loss: 0.00154 | Val Loss: 0.00166\n",
      "Epoch 046: | Train Loss: 0.00284 | Val Loss: 0.00359\n",
      "Epoch 047: | Train Loss: 0.00161 | Val Loss: 0.00185\n",
      "Epoch 048: | Train Loss: 0.00120 | Val Loss: 0.00250\n",
      "Epoch 049: | Train Loss: 0.00126 | Val Loss: 0.00087\n",
      "Epoch 050: | Train Loss: 0.00353 | Val Loss: 0.00100\n",
      "Epoch 051: | Train Loss: 0.00114 | Val Loss: 0.00076\n",
      "Epoch 052: | Train Loss: 0.00124 | Val Loss: 0.00074\n",
      "Epoch 053: | Train Loss: 0.00106 | Val Loss: 0.00108\n",
      "Epoch 054: | Train Loss: 0.00171 | Val Loss: 0.00129\n",
      "Epoch 055: | Train Loss: 0.00136 | Val Loss: 0.00111\n",
      "Epoch 056: | Train Loss: 0.00136 | Val Loss: 0.00199\n",
      "Epoch 057: | Train Loss: 0.00161 | Val Loss: 0.00100\n",
      "Epoch 058: | Train Loss: 0.00150 | Val Loss: 0.00401\n",
      "Epoch 059: | Train Loss: 0.00122 | Val Loss: 0.00194\n",
      "Epoch 060: | Train Loss: 0.00106 | Val Loss: 0.00058\n",
      "Epoch 061: | Train Loss: 0.00128 | Val Loss: 0.00103\n",
      "Epoch 062: | Train Loss: 0.00230 | Val Loss: 0.00108\n",
      "Epoch 063: | Train Loss: 0.00153 | Val Loss: 0.00158\n",
      "Epoch 064: | Train Loss: 0.00115 | Val Loss: 0.00068\n",
      "Epoch 065: | Train Loss: 0.00108 | Val Loss: 0.00199\n",
      "Epoch 066: | Train Loss: 0.00094 | Val Loss: 0.00047\n",
      "Epoch 067: | Train Loss: 0.00098 | Val Loss: 0.00105\n",
      "Epoch 068: | Train Loss: 0.00190 | Val Loss: 0.00392\n",
      "Epoch 069: | Train Loss: 0.00118 | Val Loss: 0.00048\n",
      "Epoch 070: | Train Loss: 0.00123 | Val Loss: 0.00039\n",
      "Epoch 071: | Train Loss: 0.00136 | Val Loss: 0.00065\n",
      "Epoch 072: | Train Loss: 0.00157 | Val Loss: 0.00065\n",
      "Epoch 073: | Train Loss: 0.00478 | Val Loss: 0.00134\n",
      "Epoch 074: | Train Loss: 0.00082 | Val Loss: 0.00127\n",
      "Epoch 075: | Train Loss: 0.00066 | Val Loss: 0.00093\n",
      "Epoch 076: | Train Loss: 0.00091 | Val Loss: 0.00039\n",
      "Epoch 077: | Train Loss: 0.00090 | Val Loss: 0.00049\n",
      "Epoch 078: | Train Loss: 0.00136 | Val Loss: 0.00070\n",
      "Epoch 079: | Train Loss: 0.00314 | Val Loss: 0.00066\n",
      "Epoch 080: | Train Loss: 0.00098 | Val Loss: 0.00097\n",
      "Epoch 081: | Train Loss: 0.00102 | Val Loss: 0.00075\n",
      "Epoch 082: | Train Loss: 0.00112 | Val Loss: 0.00056\n",
      "Epoch 083: | Train Loss: 0.00081 | Val Loss: 0.00106\n",
      "Epoch 084: | Train Loss: 0.00118 | Val Loss: 0.00054\n",
      "Epoch 085: | Train Loss: 0.00104 | Val Loss: 0.00057\n",
      "Epoch 086: | Train Loss: 0.00073 | Val Loss: 0.01017\n",
      "Epoch 087: | Train Loss: 0.00117 | Val Loss: 0.00044\n",
      "Epoch 088: | Train Loss: 0.00074 | Val Loss: 0.00049\n",
      "Epoch 089: | Train Loss: 0.00142 | Val Loss: 0.00037\n",
      "Epoch 090: | Train Loss: 0.00118 | Val Loss: 0.00100\n",
      "Epoch 091: | Train Loss: 0.00078 | Val Loss: 0.00027\n",
      "Epoch 092: | Train Loss: 0.00067 | Val Loss: 0.00062\n",
      "Epoch 093: | Train Loss: 0.00128 | Val Loss: 0.00033\n",
      "Epoch 094: | Train Loss: 0.00065 | Val Loss: 0.00070\n",
      "Epoch 095: | Train Loss: 0.00096 | Val Loss: 0.00144\n",
      "Epoch 096: | Train Loss: 0.00085 | Val Loss: 0.00034\n",
      "Epoch 097: | Train Loss: 0.00200 | Val Loss: 0.00059\n",
      "Epoch 098: | Train Loss: 0.00047 | Val Loss: 0.00035\n",
      "Epoch 099: | Train Loss: 0.00106 | Val Loss: 0.00039\n",
      "Epoch 100: | Train Loss: 0.00085 | Val Loss: 0.00062\n",
      "Epoch 101: | Train Loss: 0.00269 | Val Loss: 0.00039\n",
      "Epoch 102: | Train Loss: 0.00045 | Val Loss: 0.00087\n",
      "Epoch 103: | Train Loss: 0.00060 | Val Loss: 0.00035\n",
      "Epoch 104: | Train Loss: 0.00133 | Val Loss: 0.00133\n",
      "Epoch 105: | Train Loss: 0.00117 | Val Loss: 0.00026\n",
      "Epoch 106: | Train Loss: 0.00116 | Val Loss: 0.00043\n",
      "Epoch 107: | Train Loss: 0.00302 | Val Loss: 0.00045\n",
      "Epoch 108: | Train Loss: 0.00057 | Val Loss: 0.00060\n",
      "Epoch 109: | Train Loss: 0.00038 | Val Loss: 0.00024\n",
      "Epoch 110: | Train Loss: 0.00093 | Val Loss: 0.00044\n",
      "Epoch 111: | Train Loss: 0.00080 | Val Loss: 0.00021\n",
      "Epoch 112: | Train Loss: 0.00045 | Val Loss: 0.00028\n",
      "Epoch 113: | Train Loss: 0.00114 | Val Loss: 0.00040\n",
      "Epoch 114: | Train Loss: 0.00182 | Val Loss: 0.00075\n",
      "Epoch 115: | Train Loss: 0.00041 | Val Loss: 0.00019\n",
      "Epoch 116: | Train Loss: 0.00048 | Val Loss: 0.00061\n",
      "Epoch 117: | Train Loss: 0.00066 | Val Loss: 0.00032\n",
      "Epoch 118: | Train Loss: 0.00108 | Val Loss: 0.00045\n",
      "Epoch 119: | Train Loss: 0.00108 | Val Loss: 0.00034\n",
      "Epoch 120: | Train Loss: 0.00097 | Val Loss: 0.00107\n",
      "Epoch 121: | Train Loss: 0.00094 | Val Loss: 0.00031\n",
      "Epoch 122: | Train Loss: 0.00042 | Val Loss: 0.00049\n",
      "Epoch 123: | Train Loss: 0.00120 | Val Loss: 0.00221\n",
      "Epoch 124: | Train Loss: 0.00079 | Val Loss: 0.00042\n",
      "Epoch 125: | Train Loss: 0.00051 | Val Loss: 0.00044\n",
      "Epoch 126: | Train Loss: 0.00093 | Val Loss: 0.00060\n",
      "Epoch 127: | Train Loss: 0.00057 | Val Loss: 0.00116\n",
      "Epoch 128: | Train Loss: 0.00130 | Val Loss: 0.00504\n",
      "Epoch 129: | Train Loss: 0.00094 | Val Loss: 0.00074\n",
      "Epoch 130: | Train Loss: 0.00239 | Val Loss: 0.00090\n",
      "Epoch 131: | Train Loss: 0.00189 | Val Loss: 0.00043\n",
      "Epoch 132: | Train Loss: 0.00040 | Val Loss: 0.00019\n",
      "Epoch 133: | Train Loss: 0.00036 | Val Loss: 0.00018\n",
      "Epoch 134: | Train Loss: 0.00043 | Val Loss: 0.00160\n",
      "Epoch 135: | Train Loss: 0.00303 | Val Loss: 0.00030\n",
      "Epoch 136: | Train Loss: 0.00059 | Val Loss: 0.00057\n",
      "Epoch 137: | Train Loss: 0.00041 | Val Loss: 0.00095\n",
      "Epoch 138: | Train Loss: 0.00040 | Val Loss: 0.00027\n",
      "Epoch 139: | Train Loss: 0.00035 | Val Loss: 0.00155\n",
      "Epoch 140: | Train Loss: 0.00127 | Val Loss: 0.00025\n",
      "Epoch 141: | Train Loss: 0.00096 | Val Loss: 0.00584\n",
      "Epoch 142: | Train Loss: 0.00169 | Val Loss: 0.00069\n",
      "Epoch 143: | Train Loss: 0.00063 | Val Loss: 0.00103\n",
      "Epoch 144: | Train Loss: 0.00036 | Val Loss: 0.00025\n",
      "Epoch 145: | Train Loss: 0.00180 | Val Loss: 0.00047\n",
      "Epoch 146: | Train Loss: 0.00075 | Val Loss: 0.00032\n",
      "Epoch 147: | Train Loss: 0.00044 | Val Loss: 0.00017\n",
      "Epoch 148: | Train Loss: 0.00046 | Val Loss: 0.00058\n",
      "Epoch 149: | Train Loss: 0.00046 | Val Loss: 0.00248\n",
      "Epoch 150: | Train Loss: 0.00106 | Val Loss: 0.00018\n"
     ]
    }
   ],
   "source": [
    "print(\"Begin training.\")\n",
    "for e in tqdm(range(1, EPOCHS+1)):\n",
    "    \n",
    "    # TRAINING\n",
    "    train_epoch_loss = 0\n",
    "    model.train()\n",
    "    for X_train_batch, y_train_batch in train_loader:\n",
    "        X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_train_pred = model(X_train_batch)\n",
    "\n",
    "        train_loss = criterion(y_train_pred, y_train_batch.unsqueeze(1))\n",
    "\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        train_epoch_loss += train_loss.item()\n",
    "        \n",
    "        \n",
    "    # VALIDATION    \n",
    "    with torch.no_grad():\n",
    "        val_epoch_loss = 0\n",
    "        model.eval()\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "            \n",
    "            y_val_pred = model(X_val_batch)    \n",
    "            val_loss = criterion(y_val_pred, y_val_batch.unsqueeze(1))\n",
    "            val_epoch_loss += val_loss.item()\n",
    "            \n",
    "    loss_stats['train'].append(train_epoch_loss/len(train_loader))\n",
    "    loss_stats['val'].append(val_epoch_loss/len(val_loader))                              \n",
    "    \n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {train_epoch_loss/len(train_loader):.5f} | Val Loss: {val_epoch_loss/len(val_loader):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9221d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Train-Val Loss/Epoch')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_val_loss_df = pd.DataFrame.from_dict(loss_stats).reset_index().melt(id_vars=['index']).rename(columns={\"index\":\"epochs\"})\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.lineplot(data=train_val_loss_df, x = \"epochs\", y=\"value\", hue=\"variable\").set_title('Train-Val Loss/Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b441d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for X_batch, _ in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_pred_list.append(y_test_pred.cpu().numpy())\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4691807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error : 0.00019035943732544972\n",
      "R^2 : 0.9998372588629642\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_test, y_pred_list)\n",
    "r_square = r2_score(y_test, y_pred_list)\n",
    "print(\"Mean Squared Error :\",mse)\n",
    "print(\"R^2 :\",r_square)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce77f44",
   "metadata": {},
   "source": [
    "#### Predicting for a new row of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35dfdc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = [[-0.186135, -0.167322, 211, -0.183676, 0, 0, 0, 0, 0, 1],\n",
    "    [-0.171125, -0.177002, 211, -0.203476, 0, 0, 0, 0, 0, 1]]\n",
    "\n",
    "# T = np.array([np.array(xi) for xi in T])\n",
    "T = RegressionDataset(torch.from_numpy(np.array([np.array(xi) for xi in T])).float(), torch.from_numpy(y_train).float())\n",
    "\n",
    "t_loader = DataLoader(dataset=T, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "y_pred_list = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for X_batch, _ in t_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_pred_list.append(y_test_pred.cpu().numpy())\n",
    "        \n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "915c9891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.1796792894601822, -0.1834317147731781]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ce429f",
   "metadata": {},
   "source": [
    "### 4. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e683236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is all columns except target var and y is target var\n",
    "X = df_others.drop([\"ecoFootprint\"], 1)\n",
    "y = df_others[\"ecoFootprint\"]\n",
    "y = np.asarray(y).astype('float32')\n",
    "\n",
    "# Split train into train-val\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fecbd4b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.19146031, -0.19216598, -0.01648922, ..., -0.19174319,\n",
       "       -0.19219363,  0.05137983])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cdbbd8d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8060"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2b7b26a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error : 0.00040444018359855714\n",
      "R^2 : 0.9995876937381284\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r_square = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error :\",mse)\n",
    "print(\"R^2 :\",r_square)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e507fa8",
   "metadata": {},
   "source": [
    "### 5. Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9cbd3900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor(max_depth = 1)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = tree.predict(X_test).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8720ab5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error : 0.44230200385051377\n",
      "R^2 : 0.5490955319935955\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_test, y_pred_train)\n",
    "r_square = r2_score(y_test, y_pred_train)\n",
    "\n",
    "print(\"Mean Squared Error :\",mse)\n",
    "print(\"R^2 :\",r_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1434b748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa08a3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
